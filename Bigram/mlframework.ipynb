{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1356048"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights, Inputs, Transformer (layers of MLP)\n",
    "\n",
    "words = open('names.txt').read().splitlines()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". ML model write\n",
    "2. Explain what happens inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2]), torch.Size([6, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes x characters, predicts next\n",
    "\n",
    "blockSize = 2\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  w = ['.'] * blockSize + list(w) + ['.']\n",
    "  arr = []\n",
    "  for c1, c2, pred in zip(w, w[1:], w[2:]):\n",
    "    c1 = stoi[c1]\n",
    "    c2 = stoi[c2]\n",
    "    pred = stoi[pred]\n",
    "    X.append([c1, c2])\n",
    "    Y.append([pred])\n",
    "\n",
    "torch.tensor(X, dtype=\"cuda\").shape, torch.tensor(Y).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP in pytorch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.layer = config.layer1 #27\n",
    "    self.layers = config.layers #27,27\n",
    "    # Init weights as well\n",
    "    # Random distribution : PRNG : HIGH CHances there are memory allocation on the device ( backend)\n",
    "# mlx: __call__\n",
    "# pytorch: \n",
    "  def forward(self):\n",
    "    # logits = W@layers + bias \n",
    "    # Activation function (softmax / tanh/ relu/ sigmoid)\n",
    "    # Loss = \n",
    "    return loss\n",
    "\n",
    "model = MLP(config)\n",
    "loss = model.eval(Inputs)\n",
    "# weights = Weights - loss * learningRate \n",
    "\n",
    "error =  loss.backward()\n",
    "weights = weights - loss * learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing the execution of framework: \n",
    "\n",
    "Profiling: \n",
    "1. Static analysis: PGO\n",
    "\n",
    "2. Dynamic analysis: Code is already running\n",
    "Specific Graph is running, and now, how do we optimize this.\n",
    "\n",
    "Selectively replace a small region of the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch network: compare Nvidia hw vs Google hw\n",
    "1 workload across different hardware\n",
    "\n",
    "How to evaluate this?\n",
    "\n",
    "Which layer in Pytorch could enable us to do that?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
